{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snowpark for Python\n",
    "\n",
    "This is a simple example of getting Snowpark up and running.<br>\n",
    "\n",
    "For performing activities with snowflake/snowpark & Python, install the follow packages: \n",
    "\n",
    "- snowflake-connector-python\n",
    "- snowflake-snowpark-python\n",
    "- snowflake-sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "from snowflake.snowpark import Session\n",
    "from snowflake.snowpark.functions import col, when, lit, explode,split,\\\n",
    "    replace, substring, charindex, array_agg, object_construct_keep_null # example funcs to import & use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary containing your Snowflake connection configs. Typically, I would usually have this in an environment file or config file that does not get committed to any version history. This will avoid passwords within code etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowConn = {\n",
    "    'account': '',\n",
    "    'user': 'my_username',\n",
    "    'password': 'my_password',\n",
    "    'role': 'my_de_role',\n",
    "    'warehouse': 'my_warehouse',\n",
    "    'database': 'my_database',\n",
    "    'schema': 'my_schema'\n",
    "    #,'authenticator': 'externalbrowser' # This may be needed if you connect to Snowflake UI via SSO\n",
    "}\n",
    "\n",
    "try:\n",
    "    # this will create the `snowpark` object that refers to your remote snowflake compute\n",
    "    snowpark = Session.builder.configs(snowConn).create()\n",
    "    print(\"Snowpark is now available!\")\n",
    "except Exception as e:\n",
    "    print(e) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a really basic example, assume you had a pandas data frame, locally, from reading a CSV file. You can read this pandas df into a Snowpark df, and you could then write it to Snowflake!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this reads a fake CSV file of data\n",
    "file = \"fake_data.csv\"\n",
    "local_data = pd.read_csv(file)\n",
    "\n",
    "# now push the pandas DF to a snowpark DF\n",
    "snow_df = snowpark.create_dataframe(local_data)\n",
    "\n",
    "# maybe we want to add a new column, a simple literal string for instance\n",
    "# this could be \"source file\", so we know where the original data came from\n",
    "snow_df_new = snow_df.with_column('SOURCE_FILE', lit(str(file)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create views of these dataframes (as temp in session memory, so until the snowpark session terminates) which would allow snowflake SQL to be performed on the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create temp view - call it MY_DATA\n",
    "snow_df_new.create_or_replace_temp_view(\"MY_DATA\")\n",
    "\n",
    "# now we can build another dataframe, that aggregates data, but this time, use SQL!\n",
    "aggQuery = \"\"\"\n",
    "SELECT\n",
    "    CLASS,\n",
    "    COUNT(NAME) AS NO_OF_STUDENTS,\n",
    "    SUM(SCORE_100) AS TOTAL_SCORE_CLASS\n",
    "FROM MY_DATA\n",
    "GROUP BY CLASS\n",
    "ORDER BY CLASS                       \n",
    "\"\"\"\n",
    "agg_data = snowpark.sql(aggQuery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a dataframe that is an aggregate of our data!<br>\n",
    "Maybe, we want to save that to a table in snowflake. To do this, we could use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write data to snowflake table\n",
    "agg_data.write.mode(\"overwrite\").save_as_table(\"PUBLIC.AGGREGATED_DATA_TEST\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once that completes, you would now be able to query it with Snowpark again! For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display class A only\n",
    "agg_class_a = snowpark.sql(\"SELECT * FROM PUBLIC.AGGREGATED_DATA_TEST WHERE CLASS = 'A'\")\n",
    "\n",
    "# we can actually use the toPandas() method to bring the DF results back to local memory & display via pandas\n",
    "agg_class_a.toPandas() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now close the session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close snowpark session \n",
    "snowpark.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can obviously do way more complex workloads here, and the syntax is pretty much 99% compaitble with PySpark (the Python API for Spark, which is super helpful as it has a better established online community atm) <br>\n",
    "\n",
    "Docs can be found at: https://docs.snowflake.com/en/developer-guide/snowpark/reference/python/latest/index "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dmc1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
