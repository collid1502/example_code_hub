{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring PySpark\n",
    "\n",
    "This notebook covers some details on PySpark, and shows example code of how you can perform different functions / actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. What is Apache Spark?\n",
    "\n",
    "Spark is a big data processing framework (open-source), that improves upon previous Hadoop Map-Reduce solutions that existed, like Hive, by processing data **in-memory** and distributing the tasks amongst multiple workers (nodes), whilst being controlled and co-oridnated by a driver. This means smaller chuncks of data can be read, processed, and then collected together when needed, to perform data transformation or analytics.\n",
    "\n",
    "The nodes in a cluster are abstracted, which means the individual nodes are not addressed directly.\n",
    "\n",
    "#### 1. So what is `PySpark`?\n",
    "\n",
    "PySpark is the Python API for Spark, which is originally written in/for Scala. It allows for the benefit of using Python with Spark, which is easier to pick up than a langauge such as Scala. This means you can you can use spark within your python scripting to leverage big data, and uncertake your transformations, analytics, ML etc.\n",
    "\n",
    "#### 2. The Difference between `Local`, `Client` and `Cluster` mode\n",
    "\n",
    "Local mode is simply running spark locally, like on your laptop. There is no submission to a cluster of machines. Only used for small, local development with limited data sizes.\n",
    "\n",
    "In Client mode, the driver runs on the machine from where the spark application is submitted, so it could be your local laptop for example. The driver still coordinates & executes tasks on the cluster. The Client sends tasks to the cluster and takes results back. Its a good mode for development, and or debugging applications. It also provides easier access to logs from the application.\n",
    "\n",
    "Cluster mode is where the driver is actually a node from the cluster, as well as the actual workers.<br>\n",
    "The driver still does the same tasks.<br>\n",
    "The client machine submits the Spark application to the cluster manager, and the cluster manager takes care of running the application on the cluster.<br>\n",
    "This mode is suitable for production environments where the cluster manager handles resource allocation and scheduling of tasks.\n",
    "\n",
    "#### 3. What is Fault Tolerance?\n",
    "\n",
    "Spark ensures fault tolerance of data through it's lineage information. That allows it to recompute lost data rather than replicating that data across multiple nodes. At the core of fault tolerance is the RDDs (Resilient Distributed Datasets). RDDs are low-level, immutable and fault-tolerant collections of data that can be processed in parallel across a cluster. By maintaining the lineage of transformations applied to the data, it can re-compute that partition of lost data as needed, and thus not have to reprocess the entire dataset.\n",
    "\n",
    "RDD lineage is represented as a DAG (Directed Acyclic Graph) of all the transformations applied to the base RDD. \n",
    "\n",
    "#### 4. RDD vs DataFrame ?\n",
    "\n",
    "An RDD and a DataFrame are both storage organisation strategies for used by Apache Spark. An RDD is a collection of objects (data objects) across multiple nodes in a Spark cluster. A DataFrame is more similar to a standard SQL Database table, where an overlying schema on the data lays it out into columns and rows. The DataFrame API is useful, as with data laid out in columns, queries can be optimized for performance. \n",
    "\n",
    "An RDD distributes data across partitions across multiple nodes (servers etc.) as unstructured blocks of data. As this data is immutable, its never updated, but is recreated when changes are made. \n",
    "\n",
    "A DataFrame can take an RDD and add the schema structure to it.<br> \n",
    "Typically a DataFrame is best used for structured data (though can also be used for unstructured when needed), where as RDDs are more often used for Unstructured data (so you don't know the schemas etc that your data should conform to). \n",
    "\n",
    "`Datasets` : These offer a balance between the two.\n",
    "\n",
    "Basically, DataFrames and Datasets are built upon RDDs, which is a core component of Spark.\n",
    "\n",
    "#### 5. Performance : Spark 2.x vs Spark 3.x \n",
    "\n",
    "It used to be the case that, in Spark versions 2.x.x it was typically faster to use Scala over the Python or SQL APIs. Since Spark 3.x that is no longer true. Python & SQL, with the use of dataframes can even be faster now in some cases, or at the least, performance differences are negligible.\n",
    "\n",
    "#### 6. What is Lazy Evaluation in Spark?\n",
    "\n",
    "This is where as code runs, spark is not executing the transformations until an action is called, but instead building an execution plan to process the transformations in the most optimised way it can. Since Spark 3.0, it can also include measures now to rectify potential skew in the distributed data. But, as an action is called, for an example like a count() or a show(), or write data somewhere, then it executes the transformations and steps built into its plan, and thus its lazy evaluation. \n",
    "\n",
    "#### 7. What is Shuffling?\n",
    "\n",
    "This is where, as we perform certain transformations or actions, data from across different nodes, needs to be moved to the same node as other corresponding data in order to perform the task, for example, joins, or group bys etc. This is known as shuffling, as data needs to move between nodes (shuffled) across the network. Shuffling isnt always avoidable, but steps can be taken to reduce it / minimise it. Things like bucketing and sorting (so you shuffle the data once upfront on certain key join keys you will often use, for example a Customer ID), can be used to have similar data in the same nodes and avoid future shuffling in your execution plan.\n",
    "\n",
    "#### 8. YARN and why its used\n",
    "\n",
    "YARN (Yet Another Resource Negotiator) is a resource management layer that serves as the cluster manager, so Spark can manage resources and schedule applications etc.<br>\n",
    "\n",
    "This scheduler is in the form of identifying applications, jobs/tasks within them, what resources they need from the cluster, and are those resources available as they could be used by other competing jobs. For example, as tasks in one job finish that may release resources back to the cluster, then those resources can be targeted to another job in a queue waiting for them.\n",
    "\n",
    "The application manager from YARN manages the acceptance, restart and completion of applications in the cluster. The driver acts as the application master in YARN mode. The ResourceManager allocates containers across various nodes in the cluster. The NodeManagers on those nodes launch the executors within those containers. The Spark driver coordinates with the executors to execute tasks. Executors process data and perform computations, storing intermediate results in memory or on disk as needed.\n",
    "\n",
    "#### 9. Out of Memory Errors in Spark \n",
    "\n",
    "This is viewed through two lenses.<br>\n",
    "\n",
    "The Driver, and the Executors. If the driver faces memory issues, which can happen during actions like `collect()` which pull all the data back to the driver node, is when the driver node does not have enough memory specified in the configs to hold the data in memory. Thus, your options would be to increase driver memory, or make code changes to work with smaller subsets of the data before pulling back to the driver.\n",
    "\n",
    "For Executor memory, there can be a few different causes, but often it can be down to YARN memory overhead. This is where a portion of an executors memory is dedicated to off-heap storage. It stores things liike internal strings, internal objects, or objects for non scala languages lwhen using R or Python etc. So, if you see an error like `YARN killed the container due to memory limits` this might mean you need to reconfigure the (default) settings for how much executor memory is reserved for YARN memory.\n",
    "\n",
    "High concurrency errors are where too many cores are assigned to each executor. If you have multiple cores on an executor, the memory of that executor is divided amongst them. This can potentially lead to memory exhaustion. The general guidelines for Apache Spark are four or five cores per executor, so that a machine's capacity isnt exhausted. \n",
    "\n",
    "Large partitions can also produce this issue. When a partition of your data, is significantly larger than others, it can lead to these issues. You may need to consider turning larger partitions into smaller ones, or increasing memory of the executors. \n",
    "\n",
    "#### 10. Debugging slow applications \n",
    "\n",
    "When you have a spark application that is performing slowly, there's a good chance a bottleneck exists somewhere.<br>\n",
    "You can use the Spark UI and the Logs to try and identify which parts of the execution plan of the jobs are taking the longer time. You can follow the statistics captured on those tasks and use those from the UI to help debug.\n",
    "\n",
    "Typically, this can help identify where one particular task could be taking a long time (larger partition compared to rest fo data), or where lots of shuffling might be occuring. You can then go back to your code, and look at things like calling an action after a certain transformation in the code, so you can help pinpoint issues, and then work on the code optimisation in the right place. \n",
    "\n",
    "Equally, you may want to use a UI for the cluster manager too, because it may be that your application is not getting the resources it requires, and is simply hanging in a accepted state, rather than running. The logs can help identify this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. Starting a PySpark session interactively from the shell\n",
    "\n",
    "So, in your shell, you can execute<br>\n",
    "```\n",
    "pyspark\n",
    "```\n",
    "which would start an interactive PySpark shell (assuming the relative installs and configs are place)<br>\n",
    "\n",
    "<img src=\"./images/pyspark_shell.png\">\n",
    "\n",
    "You can then exit this shell using:\n",
    "\n",
    "```\n",
    "exit()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12. Creating a local PySpark session in your Python Code\n",
    "\n",
    "Note, when starting a spark session, it can take up to a few minutes at times to launch, depending on the setup being used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "24/05/15 14:58:11 WARN Utils: Your hostname, DCollins-Laptop1 resolves to a loopback address: 127.0.1.1; using 172.26.39.146 instead (on interface eth0)\n",
      "24/05/15 14:58:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/15 14:58:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4.1\n"
     ]
    }
   ],
   "source": [
    "# imports \n",
    "from pyspark.sql import SparkSession \n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "            .appName(\"my_local_spark_session\")\\\n",
    "            .master(\"local\")\\\n",
    "            .getOrCreate() \n",
    "\n",
    "# let's print the details of our local spark session \n",
    "print(spark.version) \n",
    "\n",
    "# close the spark session \n",
    "spark.stop() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13. Spark Context\n",
    "\n",
    "Spark Context establishes a connection to the spark cluster. It can connect to various cluster managers like YARN, Mesos, or a standalone Spark cluster. <br>\n",
    "\n",
    "It's responsible for submitting jobs to the cluster, and handles the scheduling and distribution of tasks from your application to the cluster. Equally, it holds the configuration management of your spark application running on the cluster. \n",
    "\n",
    "It can be used to created RDDs etc. \n",
    "\n",
    "Basically, think of it as the main entry point for spark functionality, and was the traditional approach. SparkSession (introduced in Spark 2.x) is a unified interface that combines Spark's various functionalities into a single entry point. SparkSession integrates SparkContext and provides a higher level API for working with Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/15 12:33:24 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark UI: http://172.26.39.146:4041\n",
      "Spark Application ID: local-1715772804723\n",
      "Spark App Start Time: 1715772804647\n",
      "Spark default Paralleselism: 1\n",
      "Spark default min partitions: 1\n",
      "====================================================================================================\n",
      "<bound method SparkContext.statusTracker of <SparkContext master=local appName=my_local_spark_session>>\n"
     ]
    }
   ],
   "source": [
    "# imports \n",
    "from pyspark.sql import SparkSession \n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "            .appName(\"my_local_spark_session\")\\\n",
    "            .master(\"local\")\\\n",
    "            .getOrCreate() \n",
    "\n",
    "sc = spark.sparkContext \n",
    "\n",
    "print(f\"Spark UI: {sc.uiWebUrl}\") # Url of the Spark Web UI\n",
    "print(f\"Spark Application ID: {sc.applicationId}\") # ID of the spark application \n",
    "print(f\"Spark App Start Time: {sc.startTime}\") # Start time of the application \n",
    "print(f\"Spark default Paralleselism: {sc.defaultParallelism}\") # Default parallelelism level \n",
    "print(f\"Spark default min partitions: {sc.defaultMinPartitions}\") # Default minimum partitions \n",
    "\n",
    "print(\"=\" * 100)\n",
    "# Status info\n",
    "print(sc.statusTracker)\n",
    "\n",
    "# close\n",
    "spark.stop() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 14. Creating a more complex Spark application where you need to specify configs and provide JAR files for additional functionality \n",
    "\n",
    "* You can specify configs explicitly during the session builder of your application\n",
    "* you can proivde things like JAR files or Python code to be sent to each node of the cluster during the application, so you can do additional functionality like JDBC connections to databases, or using external code like AWS Deequ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# below covers some PySpark examples, when working with a cluster etc\n",
    "# imports\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "useJarFiles = [\n",
    "    's3://my_bucket_location/folder1/jars/dummy1.jar',\n",
    "    's3://my_bucket_location/folder1/jars/dummy2.jar'\n",
    "]\n",
    "jarList = \",\".join(useJarFiles) \n",
    "\n",
    "#Python version settings, this allows us to target an executor environment to match our driver \n",
    "pyspark_deps = f\"s3://user/spark/shared/lib/pyspark4.8-deps/environment.tar.gz#environment\"\n",
    "\n",
    "# build spark session \n",
    "def getSpark(\n",
    "        appName: str,\n",
    "        driverMemory: str = \"2G\",\n",
    "        executorMemory: str = \"4G\",\n",
    "        executorCores: str = \"5\",\n",
    "        queue: str = 'default',\n",
    "        addJarFiles: list = [r\"s3://user/spark/jdbc/jarsFiles/postgresql-42.6.0.jar\"]\n",
    ") -> object:\n",
    "    \"\"\"\n",
    "    Simple function to return a spark session through object `spark`\n",
    "    \"\"\"\n",
    "    spark = SparkSession\\\n",
    "            .builder\\\n",
    "            .appName(appName)\\\n",
    "            .enableHiveSupport()\\\n",
    "            .master(\"yarn\")\\\n",
    "            .config(\"spark.driver.memory\", driverMemory)\\\n",
    "            .config(\"spark.executor.memory\", executorMemory)\\\n",
    "            .config(\"spark.yarn.queue\", queue)\\\n",
    "            .config(\"spark.dynamicAllocation.enabled\", \"true\")\\\n",
    "            .config(\"spark.dynamicAllocation.initialExecutors\", \"0\")\\\n",
    "            .config(\"spark.dynamicAllocation.maxExecutors\", \"16\")\\\n",
    "            .config(\"spark.dynamicAllocation.minExecutors\", \"1\")\\\n",
    "            .config(\"spark.executors.cores\", executorCores)\\\n",
    "            .config(\"spark.sql.hive.caseSensitiveInferenceMode\", \"INFER_ONLY\")\\\n",
    "            .config(\"spark.sql.caseSensitive\", \"false\")\\\n",
    "            .config(\"spark.sql.parquet.writeLegacyFormat\", \"true\")\\\n",
    "            .config(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\\\n",
    "            .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\")\\\n",
    "            .config(\"spark.shuffle.service.enabled\", \"true\")\\\n",
    "            .config(\"spark.dynamicAllocation.InitialExecutors\", \"0\")\\\n",
    "            .config(\"spark.yarn.dist.archives\", pyspark_deps)\\\n",
    "            .config(\"spark.jars\", addJarFiles)\\\n",
    "            .getOrCreate()\n",
    "    return spark\n",
    "\n",
    "\n",
    "try:\n",
    "    spark = getSpark(appName='Pyspark_EMR_Test') \n",
    "    print(\"PySpark session available through `spark` object\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# show databases \n",
    "databases = spark.sql(\"SHOW DATABASES\")\n",
    "databases.toPandas() \n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15. Reading in CSV files to a Spark DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "24/05/15 15:01:33 WARN Utils: Your hostname, DCollins-Laptop1 resolves to a loopback address: 127.0.1.1; using 172.26.39.146 instead (on interface eth0)\n",
      "24/05/15 15:01:33 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/15 15:01:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# import \n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# create session \n",
    "spark = SparkSession.builder\\\n",
    "            .appName(\"my_local_spark_session\")\\\n",
    "            .master(\"local\")\\\n",
    "            .getOrCreate() \n",
    "sc = spark.sparkContext "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, from the `/data` subfolder, read in the customer CSV file to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------+-------------+------------------------+--------+-------------------+----------+-------------------+\n",
      "|customerID|firstName|lastName|rewardsMember|emailAddress            |postcode|profession         |dob       |customerJoined     |\n",
      "+----------+---------+--------+-------------+------------------------+--------+-------------------+----------+-------------------+\n",
      "|10000     |Helen    |Hope    |true         |gordon49@example.com    |N16 8GZ |Quantity surveyor  |1962-09-18|2002-01-28 13:27:36|\n",
      "|10001     |George   |Hill    |false        |kgrant@example.org      |E35 0TP |Graphic Designer   |1984-10-02|2023-04-22 02:00:33|\n",
      "|10002     |Hollie   |Morris  |true         |singhben@example.net    |M16 9GR |Roofer             |1985-08-19|2010-04-12 19:25:23|\n",
      "|10003     |Carolyn  |Johnston|true         |barnesdawn@example.org  |TR8X 4YS|Pharmacy Technician|1958-11-02|1991-06-01 05:33:14|\n",
      "|10004     |Roger    |Atkins  |true         |bernardstone@example.com|S50 0TD |Archaeologist      |2004-05-18|2021-04-28 10:38:56|\n",
      "+----------+---------+--------+-------------+------------------------+--------+-------------------+----------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filePath = \"./data/customerMasterExtract.csv\"\n",
    "customer_df = (spark.read\n",
    "    .option(\"delimiter\", \",\") # sets the delimiter to `,`\n",
    "    .option(\"header\", \"true\") # informs pyspark that row 1 should be treated as the column headings of the data \n",
    "    .option(\"inferSchema\", \"true\") # lets spark infer the schema of the data itself, rather than us expliitly creating a schema\n",
    "    .csv(filePath) \n",
    ")\n",
    "\n",
    "customer_df.show(5, truncate=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 16. Creating a new column with a literal value\n",
    "\n",
    "This is where you create a new column in the data, where you want all rows to have the same value.<br>\n",
    "For example, lets take the above data frame and add a simple column called \"cardHolder\" and give every customer a default value of 'Y'\n",
    "\n",
    "To do this, we need to import some extra PySpark methods/functions\n",
    "\n",
    "*NOTE - In an actual script, you would do this all at the top, but for this demo, don't worry*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------+-------------+--------------------+--------+-----------------+----------+-------------------+----------+\n",
      "|customerID|firstName|lastName|rewardsMember|emailAddress        |postcode|profession       |dob       |customerJoined     |cardHolder|\n",
      "+----------+---------+--------+-------------+--------------------+--------+-----------------+----------+-------------------+----------+\n",
      "|10000     |Helen    |Hope    |true         |gordon49@example.com|N16 8GZ |Quantity surveyor|1962-09-18|2002-01-28 13:27:36|Y         |\n",
      "|10001     |George   |Hill    |false        |kgrant@example.org  |E35 0TP |Graphic Designer |1984-10-02|2023-04-22 02:00:33|Y         |\n",
      "|10002     |Hollie   |Morris  |true         |singhben@example.net|M16 9GR |Roofer           |1985-08-19|2010-04-12 19:25:23|Y         |\n",
      "+----------+---------+--------+-------------+--------------------+--------+-----------------+----------+-------------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit \n",
    "\n",
    "card_holder_customers = (customer_df   # specifies the base dataframe to transform from \n",
    "    .withColumn(                       # uses withColumn to create a new column \n",
    "        \"cardHolder\",                  # passes \"cardHolder\" as the new column name\n",
    "        lit(\"Y\")                       # gives each row the literal value 'Y' with the lit() method\n",
    "    )\n",
    ")\n",
    "\n",
    "card_holder_customers.show(3, truncate=False) # using .show() calls an `action` which actually executes the transformation above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 17. Re-partition Data\n",
    "\n",
    "We can explore how many partitions our data has, and we can re-partition it where required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# current number of partitions \n",
    "print(card_holder_customers.rdd.getNumPartitions()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say, we want to partition on `profession`, which has relatively low cardinality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "custs_partitioned_by_rewards = (\n",
    "    card_holder_customers.repartition(\"profession\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
