{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring PySpark\n",
    "\n",
    "This notebook covers some details on PySpark, and shows example code of how you can perform different functions / actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. What is Apache Spark?\n",
    "\n",
    "Spark is a big data processing framework (open-source), that improves upon previous Hadoop Map-Reduce solutions that existed, like Hive, by processing data **in-memory** and distributing the tasks amongst multiple workers (nodes), whilst being controlled and co-oridnated by a driver. This means smaller chuncks of data can be read, processed, and then collected together when needed, to perform data transformation or analytics.\n",
    "\n",
    "The nodes in a cluster are abstracted, which means the individual nodes are not addressed directly.\n",
    "\n",
    "#### 1. So what is `PySpark`?\n",
    "\n",
    "PySpark is the Python API for Spark, which is originally written in/for Scala. It allows for the benefit of using Python with Spark, which is easier to pick up than a langauge such as Scala. This means you can you can use spark within your python scripting to leverage big data, and uncertake your transformations, analytics, ML etc.\n",
    "\n",
    "#### 2. The Difference between `Local`, `Client` and `Cluster` mode\n",
    "\n",
    "Local mode is simply running spark locally, like on your laptop. There is no submission to a cluster of machines. Only used for small, local development with limited data sizes.\n",
    "\n",
    "In Client mode, the driver runs on the machine from where the spark application is submitted, so it could be your local laptop for example. The driver still coordinates & executes tasks on the cluster. The Client sends tasks to the cluster and takes results back. Its a good mode for development, and or debugging applications. It also provides easier access to logs from the application.\n",
    "\n",
    "Cluster mode is where the driver is actually a node from the cluster, as well as the actual workers.<br>\n",
    "The driver still does the same tasks.<br>\n",
    "The client machine submits the Spark application to the cluster manager, and the cluster manager takes care of running the application on the cluster.<br>\n",
    "This mode is suitable for production environments where the cluster manager handles resource allocation and scheduling of tasks.\n",
    "\n",
    "#### 3. What is Fault Tolerance?\n",
    "\n",
    "Spark ensures fault tolerance of data through it's lineage information. That allows it to recompute lost data rather than replicating that data across multiple nodes. At the core of fault tolerance is the RDDs (Resilient Distributed Datasets). RDDs are low-level, immutable and fault-tolerant collections of data that can be processed in parallel across a cluster. By maintaining the lineage of transformations applied to the data, it can re-compute that partition of lost data as needed, and thus not have to reprocess the entire dataset.\n",
    "\n",
    "RDD lineage is represented as a DAG (Directed Acyclic Graph) of all the transformations applied to the base RDD. \n",
    "\n",
    "#### 4. RDD vs DataFrame ?\n",
    "\n",
    "An RDD and a DataFrame are both storage organisation strategies for used by Apache Spark. An RDD is a collection of objects (data objects) across multiple nodes in a Spark cluster. A DataFrame is more similar to a standard SQL Database table, where an overlying schema on the data lays it out into columns and rows. The DataFrame API is useful, as with data laid out in columns, queries can be optimized for performance. \n",
    "\n",
    "An RDD distributes data across partitions across multiple nodes (servers etc.) as unstructured blocks of data. As this data is immutable, its never updated, but is recreated when changes are made. \n",
    "\n",
    "A DataFrame can take an RDD and add the schema structure to it.<br> \n",
    "Typically a DataFrame is best used for structured data (though can also be used for unstructured when needed), where as RDDs are more often used for Unstructured data (so you don't know the schemas etc that your data should conform to). \n",
    "\n",
    "`Datasets` : These offer a balance between the two.\n",
    "\n",
    "Basically, DataFrames and Datasets are built upon RDDs, which is a core component of Spark.\n",
    "\n",
    "#### 5. Performance : Spark 2.x vs Spark 3.x \n",
    "\n",
    "It used to be the case that, in Spark versions 2.x.x it was typically faster to use Scala over the Python or SQL APIs. Since Spark 3.x that is no longer true. Python & SQL, with the use of dataframes can even be faster now in some cases, or at the least, performance differences are negligible.\n",
    "\n",
    "#### 6. What is Lazy Evaluation in Spark?\n",
    "\n",
    "This is where as code runs, spark is not executing the transformations until an action is called, but instead building an execution plan to process the transformations in the most optimised way it can. Since Spark 3.0, it can also include measures now to rectify potential skew in the distributed data. But, as an action is called, for an example like a count() or a show(), or write data somewhere, then it executes the transformations and steps built into its plan, and thus its lazy evaluation. \n",
    "\n",
    "#### 7. What is Shuffling?\n",
    "\n",
    "This is where, as we perform certain transformations or actions, data from across different nodes, needs to be moved to the same node as other corresponding data in order to perform the task, for example, joins, or group bys etc. This is known as shuffling, as data needs to move between nodes (shuffled) across the network. Shuffling isnt always avoidable, but steps can be taken to reduce it / minimise it. Things like bucketing and sorting (so you shuffle the data once upfront on certain key join keys you will often use, for example a Customer ID), can be used to have similar data in the same nodes and avoid future shuffling in your execution plan.\n",
    "\n",
    "#### 8. YARN and why its used\n",
    "\n",
    "YARN (Yet Another Resource Negotiator) is a resource management layer that serves as the cluster manager, so Spark can manage resources and schedule applications etc.<br>\n",
    "\n",
    "This scheduler is in the form of identifying applications, jobs/tasks within them, what resources they need from the cluster, and are those resources available as they could be used by other competing jobs. For example, as tasks in one job finish that may release resources back to the cluster, then those resources can be targeted to another job in a queue waiting for them.\n",
    "\n",
    "The application manager from YARN manages the acceptance, restart and completion of applications in the cluster. The driver acts as the application master in YARN mode. The ResourceManager allocates containers across various nodes in the cluster. The NodeManagers on those nodes launch the executors within those containers. The Spark driver coordinates with the executors to execute tasks. Executors process data and perform computations, storing intermediate results in memory or on disk as needed.\n",
    "\n",
    "#### 9. Out of Memory Errors in Spark \n",
    "\n",
    "This is viewed through two lenses.<br>\n",
    "\n",
    "The Driver, and the Executors. If the driver faces memory issues, which can happen during actions like `collect()` which pull all the data back to the driver node, is when the driver node does not have enough memory specified in the configs to hold the data in memory. Thus, your options would be to increase driver memory, or make code changes to work with smaller subsets of the data before pulling back to the driver.\n",
    "\n",
    "For Executor memory, there can be a few different causes, but often it can be down to YARN memory overhead. This is where a portion of an executors memory is dedicated to off-heap storage. It stores things liike internal strings, internal objects, or objects for non scala languages lwhen using R or Python etc. So, if you see an error like `YARN killed the container due to memory limits` this might mean you need to reconfigure the (default) settings for how much executor memory is reserved for YARN memory.\n",
    "\n",
    "High concurrency errors are where too many cores are assigned to each executor. If you have multiple cores on an executor, the memory of that executor is divided amongst them. This can potentially lead to memory exhaustion. The general guidelines for Apache Spark are four or five cores per executor, so that a machine's capacity isnt exhausted. \n",
    "\n",
    "Large partitions can also produce this issue. When a partition of your data, is significantly larger than others, it can lead to these issues. You may need to consider turning larger partitions into smaller ones, or increasing memory of the executors. \n",
    "\n",
    "#### 10. Debugging slow applications \n",
    "\n",
    "When you have a spark application that is performing slowly, there's a good chance a bottleneck exists somewhere.<br>\n",
    "You can use the Spark UI and the Logs to try and identify which parts of the execution plan of the jobs are taking the longer time. You can follow the statistics captured on those tasks and use those from the UI to help debug.\n",
    "\n",
    "Typically, this can help identify where one particular task could be taking a long time (larger partition compared to rest fo data), or where lots of shuffling might be occuring. You can then go back to your code, and look at things like calling an action after a certain transformation in the code, so you can help pinpoint issues, and then work on the code optimisation in the right place. \n",
    "\n",
    "Equally, you may want to use a UI for the cluster manager too, because it may be that your application is not getting the resources it requires, and is simply hanging in a accepted state, rather than running. The logs can help identify this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. Starting a PySpark session interactively from the shell\n",
    "\n",
    "So, in your shell, you can execute<br>\n",
    "```\n",
    "pyspark\n",
    "```\n",
    "which would start an interactive PySpark shell (assuming the relative installs and configs are place)<br>\n",
    "\n",
    "<img src=\"./images/pyspark_shell.png\">\n",
    "\n",
    "You can then exit this shell using:\n",
    "\n",
    "```\n",
    "exit()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12. Creating a local PySpark session in your Python Code\n",
    "\n",
    "Note, when starting a spark session, it can take up to a few minutes at times to launch, depending on the setup being used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "24/05/15 14:58:11 WARN Utils: Your hostname, DCollins-Laptop1 resolves to a loopback address: 127.0.1.1; using 172.26.39.146 instead (on interface eth0)\n",
      "24/05/15 14:58:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/15 14:58:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4.1\n"
     ]
    }
   ],
   "source": [
    "# imports \n",
    "from pyspark.sql import SparkSession \n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "            .appName(\"my_local_spark_session\")\\\n",
    "            .master(\"local\")\\\n",
    "            .getOrCreate() \n",
    "\n",
    "# let's print the details of our local spark session \n",
    "print(spark.version) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close the spark session \n",
    "spark.stop() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13. Spark Context\n",
    "\n",
    "Spark Context establishes a connection to the spark cluster. It can connect to various cluster managers like YARN, Mesos, or a standalone Spark cluster. <br>\n",
    "\n",
    "It's responsible for submitting jobs to the cluster, and handles the scheduling and distribution of tasks from your application to the cluster. Equally, it holds the configuration management of your spark application running on the cluster. \n",
    "\n",
    "It can be used to create RDDs etc. \n",
    "\n",
    "Basically, think of it as the main entry point for spark functionality, and was the traditional approach. SparkSession (introduced in Spark 2.x) is a unified interface that combines Spark's various functionalities into a single entry point. SparkSession integrates SparkContext and provides a higher level API for working with Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/15 12:33:24 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark UI: http://172.26.39.146:4041\n",
      "Spark Application ID: local-1715772804723\n",
      "Spark App Start Time: 1715772804647\n",
      "Spark default Paralleselism: 1\n",
      "Spark default min partitions: 1\n",
      "====================================================================================================\n",
      "<bound method SparkContext.statusTracker of <SparkContext master=local appName=my_local_spark_session>>\n"
     ]
    }
   ],
   "source": [
    "# imports \n",
    "from pyspark.sql import SparkSession \n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "            .appName(\"my_local_spark_session\")\\\n",
    "            .master(\"local\")\\\n",
    "            .getOrCreate() \n",
    "\n",
    "sc = spark.sparkContext \n",
    "\n",
    "print(f\"Spark UI: {sc.uiWebUrl}\") # Url of the Spark Web UI\n",
    "print(f\"Spark Application ID: {sc.applicationId}\") # ID of the spark application \n",
    "print(f\"Spark App Start Time: {sc.startTime}\") # Start time of the application \n",
    "print(f\"Spark default Paralleselism: {sc.defaultParallelism}\") # Default parallelelism level \n",
    "print(f\"Spark default min partitions: {sc.defaultMinPartitions}\") # Default minimum partitions \n",
    "\n",
    "print(\"=\" * 100)\n",
    "# Status info\n",
    "print(sc.statusTracker)\n",
    "\n",
    "# close\n",
    "spark.stop() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 14. Creating a more complex Spark application where you need to specify configs and provide JAR files for additional functionality \n",
    "\n",
    "* You can specify configs explicitly during the session builder of your application\n",
    "* you can proivde things like JAR files or Python code to be sent to each node of the cluster during the application, so you can do additional functionality like JDBC connections to databases, or using external code like AWS Deequ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# below covers some PySpark examples, when working with a cluster etc\n",
    "# imports\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "useJarFiles = [\n",
    "    's3://my_bucket_location/folder1/jars/dummy1.jar',\n",
    "    's3://my_bucket_location/folder1/jars/dummy2.jar'\n",
    "]\n",
    "jarList = \",\".join(useJarFiles) \n",
    "\n",
    "# Python version settings, this allows us to target an executor environment to match our driver \n",
    "pyspark_deps = f\"s3://user/spark/shared/lib/pyspark4.8-deps/environment.tar.gz#environment\"\n",
    "\n",
    "# build spark session \n",
    "def getSpark(\n",
    "        appName: str,\n",
    "        driverMemory: str = \"2G\",\n",
    "        executorMemory: str = \"4G\",\n",
    "        executorCores: str = \"5\",\n",
    "        queue: str = 'default',\n",
    "        addJarFiles: list = [r\"s3://user/spark/jdbc/jarsFiles/postgresql-42.6.0.jar\"]\n",
    ") -> object:\n",
    "    \"\"\"\n",
    "    Simple function to return a spark session through object `spark`\n",
    "    \"\"\"\n",
    "    spark = SparkSession\\\n",
    "            .builder\\\n",
    "            .appName(appName)\\\n",
    "            .enableHiveSupport()\\\n",
    "            .master(\"yarn\")\\\n",
    "            .config(\"spark.driver.memory\", driverMemory)\\\n",
    "            .config(\"spark.executor.memory\", executorMemory)\\\n",
    "            .config(\"spark.yarn.queue\", queue)\\\n",
    "            .config(\"spark.dynamicAllocation.enabled\", \"true\")\\\n",
    "            .config(\"spark.dynamicAllocation.initialExecutors\", \"0\")\\\n",
    "            .config(\"spark.dynamicAllocation.maxExecutors\", \"16\")\\\n",
    "            .config(\"spark.dynamicAllocation.minExecutors\", \"1\")\\\n",
    "            .config(\"spark.executors.cores\", executorCores)\\\n",
    "            .config(\"spark.sql.hive.caseSensitiveInferenceMode\", \"INFER_ONLY\")\\\n",
    "            .config(\"spark.sql.caseSensitive\", \"false\")\\\n",
    "            .config(\"spark.sql.parquet.writeLegacyFormat\", \"true\")\\\n",
    "            .config(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\\\n",
    "            .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\")\\\n",
    "            .config(\"spark.shuffle.service.enabled\", \"true\")\\\n",
    "            .config(\"spark.dynamicAllocation.InitialExecutors\", \"0\")\\\n",
    "            .config(\"spark.yarn.dist.archives\", pyspark_deps)\\\n",
    "            .config(\"spark.jars\", addJarFiles)\\\n",
    "            .getOrCreate()\n",
    "    return spark\n",
    "\n",
    "\n",
    "try:\n",
    "    spark = getSpark(appName='Pyspark_EMR_Test') \n",
    "    print(\"PySpark session available through `spark` object\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# show databases \n",
    "databases = spark.sql(\"SHOW DATABASES\")\n",
    "databases.toPandas() \n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15. Reading in CSV files to a Spark DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import \n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# create session \n",
    "spark = SparkSession.builder\\\n",
    "            .appName(\"my_local_spark_session\")\\\n",
    "            .master(\"local\")\\\n",
    "            .getOrCreate() \n",
    "sc = spark.sparkContext \n",
    "\n",
    "#print(spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, from the `/data` subfolder, read in the customer CSV file to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------+-------------+------------------------+--------+-------------------+----------+-------------------+\n",
      "|customerID|firstName|lastName|rewardsMember|emailAddress            |postcode|profession         |dob       |customerJoined     |\n",
      "+----------+---------+--------+-------------+------------------------+--------+-------------------+----------+-------------------+\n",
      "|10000     |Helen    |Hope    |true         |gordon49@example.com    |N16 8GZ |Quantity surveyor  |1962-09-18|2002-01-28 13:27:36|\n",
      "|10001     |George   |Hill    |false        |kgrant@example.org      |E35 0TP |Graphic Designer   |1984-10-02|2023-04-22 02:00:33|\n",
      "|10002     |Hollie   |Morris  |true         |singhben@example.net    |M16 9GR |Roofer             |1985-08-19|2010-04-12 19:25:23|\n",
      "|10003     |Carolyn  |Johnston|true         |barnesdawn@example.org  |TR8X 4YS|Pharmacy Technician|1958-11-02|1991-06-01 05:33:14|\n",
      "|10004     |Roger    |Atkins  |true         |bernardstone@example.com|S50 0TD |Archaeologist      |2004-05-18|2021-04-28 10:38:56|\n",
      "+----------+---------+--------+-------------+------------------------+--------+-------------------+----------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filePath = \"./data/customerMasterExtract.csv\"\n",
    "customer_df = (spark.read\n",
    "    .option(\"delimiter\", \",\") # sets the delimiter to `,`\n",
    "    .option(\"header\", \"true\") # informs pyspark that row 1 should be treated as the column headings of the data \n",
    "    .option(\"inferSchema\", \"true\") # lets spark infer the schema of the data itself, rather than us expliitly creating a schema\n",
    "    .csv(filePath) \n",
    ")\n",
    "\n",
    "customer_df.show(5, truncate=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 16. Creating a new column with a literal value\n",
    "\n",
    "This is where you create a new column in the data, where you want all rows to have the same value.<br>\n",
    "For example, lets take the above data frame and add a simple column called \"cardHolder\" and give every customer a default value of 'Y'\n",
    "\n",
    "To do this, we need to import some extra PySpark methods/functions\n",
    "\n",
    "*NOTE - In an actual script, you would do this all at the top, but for this demo, don't worry*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------+-------------+--------------------+--------+-----------------+----------+-------------------+----------+\n",
      "|customerID|firstName|lastName|rewardsMember|emailAddress        |postcode|profession       |dob       |customerJoined     |cardHolder|\n",
      "+----------+---------+--------+-------------+--------------------+--------+-----------------+----------+-------------------+----------+\n",
      "|10000     |Helen    |Hope    |true         |gordon49@example.com|N16 8GZ |Quantity surveyor|1962-09-18|2002-01-28 13:27:36|Y         |\n",
      "|10001     |George   |Hill    |false        |kgrant@example.org  |E35 0TP |Graphic Designer |1984-10-02|2023-04-22 02:00:33|Y         |\n",
      "|10002     |Hollie   |Morris  |true         |singhben@example.net|M16 9GR |Roofer           |1985-08-19|2010-04-12 19:25:23|Y         |\n",
      "+----------+---------+--------+-------------+--------------------+--------+-----------------+----------+-------------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit \n",
    "\n",
    "card_holder_customers = (customer_df   # specifies the base dataframe to transform from \n",
    "    .withColumn(                       # uses withColumn to create a new column \n",
    "        \"cardHolder\",                  # passes \"cardHolder\" as the new column name\n",
    "        lit(\"Y\")                       # gives each row the literal value 'Y' with the lit() method\n",
    "    )\n",
    ")\n",
    "\n",
    "card_holder_customers.show(3, truncate=False) # using .show() calls an `action` which actually executes the transformation above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, since Version 3.3, there is now a new way to create multiple columns, under one `withColumn` method, rather than chaining individual ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------+-------------+------------------------+--------+-------------------+----------+-------------------+----------+---------+\n",
      "|customerID|firstName|lastName|rewardsMember|emailAddress            |postcode|profession         |dob       |customerJoined     |cardHolder|staticNum|\n",
      "+----------+---------+--------+-------------+------------------------+--------+-------------------+----------+-------------------+----------+---------+\n",
      "|10000     |Helen    |Hope    |true         |gordon49@example.com    |N16 8GZ |Quantity surveyor  |1962-09-18|2002-01-28 13:27:36|Y         |10       |\n",
      "|10001     |George   |Hill    |false        |kgrant@example.org      |E35 0TP |Graphic Designer   |1984-10-02|2023-04-22 02:00:33|Y         |10       |\n",
      "|10002     |Hollie   |Morris  |true         |singhben@example.net    |M16 9GR |Roofer             |1985-08-19|2010-04-12 19:25:23|Y         |10       |\n",
      "|10003     |Carolyn  |Johnston|true         |barnesdawn@example.org  |TR8X 4YS|Pharmacy Technician|1958-11-02|1991-06-01 05:33:14|Y         |10       |\n",
      "|10004     |Roger    |Atkins  |true         |bernardstone@example.com|S50 0TD |Archaeologist      |2004-05-18|2021-04-28 10:38:56|Y         |10       |\n",
      "+----------+---------+--------+-------------+------------------------+--------+-------------------+----------+-------------------+----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create multiple columns via a dictionary map of column name & value \n",
    "from pyspark.sql.functions import col \n",
    "\n",
    "card_holder_customers_again = (\n",
    "    customer_df                     # source dataframe \n",
    "    .withColumns(                   # use the `withColumns` method\n",
    "        {                           # specify a dict of column names & values as key-pairs\n",
    "            \"cardHolder\": lit(\"Y\"),\n",
    "            \"staticNum\": lit(10),\n",
    "        }\n",
    "    )\n",
    ")\n",
    "card_holder_customers_again.show(5, truncate=False) # show results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 17. Re-partition Data\n",
    "\n",
    "We can explore how many partitions our data has, and we can re-partition it where required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current number of partitions\n",
    "print(card_holder_customers.rdd.getNumPartitions()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say, we want to partition on `profession`, which has relatively low cardinality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "custs_partitioned_by_rewards = (\n",
    "    card_holder_customers.repartition(\"profession\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could even repartition by multiple columns if needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_partition_multiple = (\n",
    "    card_holder_customers.repartition(\"profession\", \"postcode\")\n",
    ") # assuming those were columns you wished to partition the data by, and they had an even enough distribution of data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also repartition to a fixed number, if you knew how many partitions you wanted, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_partition_fixed_num = (\n",
    "    card_holder_customers.repartition(5) \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 18. Joining Data\n",
    "\n",
    "Often, we will want to join two (or more) dataframes together. Here, we can look at some options that align to the traditional SQL methods many will be familiar with:\n",
    "\n",
    "* use the `customer_df` read in earlier\n",
    "* read in the dummy transactions dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------+-------------+--------------------+--------+-----------------+----------+-------------------+\n",
      "|customerID|firstName|lastName|rewardsMember|emailAddress        |postcode|profession       |dob       |customerJoined     |\n",
      "+----------+---------+--------+-------------+--------------------+--------+-----------------+----------+-------------------+\n",
      "|10000     |Helen    |Hope    |true         |gordon49@example.com|N16 8GZ |Quantity surveyor|1962-09-18|2002-01-28 13:27:36|\n",
      "|10001     |George   |Hill    |false        |kgrant@example.org  |E35 0TP |Graphic Designer |1984-10-02|2023-04-22 02:00:33|\n",
      "|10002     |Hollie   |Morris  |true         |singhben@example.net|M16 9GR |Roofer           |1985-08-19|2010-04-12 19:25:23|\n",
      "+----------+---------+--------+-------------+--------------------+--------+-----------------+----------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check this is read in \n",
    "customer_df.show(3, truncate=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+---------+------+-----+----------+\n",
      "|customerID|transaction_TS     |Product  |volume|Price|txn_amount|\n",
      "+----------+-------------------+---------+------+-----+----------+\n",
      "|142387    |2023-10-15 15:42:50|Projector|6.0   |300.0|1800.0    |\n",
      "|126774    |2023-10-15 23:40:19|Mouse    |1.0   |8.0  |8.0       |\n",
      "|26995     |2023-10-15 20:43:32|Projector|3.0   |300.0|900.0     |\n",
      "+----------+-------------------+---------+------+-----+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# transactions data \n",
    "txns_Path = \"./data/dummy_txns.csv\"\n",
    "txns_df = (spark.read\n",
    "    .option(\"delimiter\", \",\") # sets the delimiter to `,`\n",
    "    .option(\"header\", \"true\") # informs pyspark that row 1 should be treated as the column headings of the data \n",
    "    .option(\"inferSchema\", \"true\") # lets spark infer the schema of the data itself, rather than us expliitly creating a schema\n",
    "    .csv(txns_Path) \n",
    ")\n",
    "txns_df.show(3, truncate=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join type 1 - INNER (note, this is the default implementation when not explicitly specified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of customers who bought a mouse: 10511\n",
      "======================================================================================================================================================\n",
      "Example join output:\n",
      "+-------+---------+--------+--------------+\n",
      "|cust_id|firstName|lastName|product_bought|\n",
      "+-------+---------+--------+--------------+\n",
      "|12741  |Bradley  |Wilson  |Mouse         |\n",
      "|18436  |Marc     |Osborne |Mouse         |\n",
      "|18986  |Lewis    |Williams|Mouse         |\n",
      "+-------+---------+--------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# some additional needed imports\n",
    "from pyspark.sql.functions import upper, col \n",
    " \n",
    "# let's say we want to create a dataframe that has customer names of people who purchased a \"mouse\" from the two dataframes \n",
    "mouse_txns = (\n",
    "    txns_df # source data\n",
    "    .filter(upper(txns_df[\"Product\"]) == \"MOUSE\") # apply where clause to restrict rows \n",
    "    .select(\"customerID\", \"product\") # select only the columns of interest \n",
    ").distinct() # should ensure we only get back distinct records of customer id & mouse purchases \n",
    "\n",
    "# now, let's perform an inner join against the `customer_df` to pull back names of customers who purchased a mouse \n",
    "\n",
    "cust_bought_mouse = (\n",
    "    customer_df # specify dataframe 1 \n",
    "    .join(\n",
    "        mouse_txns, # specify dataframe 2 \n",
    "        customer_df[\"customerID\"] == mouse_txns[\"customerID\"], # specify the `ON` condition for the join \n",
    "        \"inner\" # specify join type \n",
    "    )\n",
    "    .select( # select only certain columns after the join for output \n",
    "        customer_df[\"customerID\"].alias(\"cust_id\"), # selects customer_id from df1, renames it to `cust_id` via alias \n",
    "        customer_df[\"firstName\"], customer_df[\"lastName\"],\n",
    "        mouse_txns[\"Product\"].alias(\"product_bought\")\n",
    "    ) \n",
    ").distinct() # ensure there are no duplicate records on the output \n",
    "\n",
    "print(f\"No. of customers who bought a mouse: {cust_bought_mouse.count()}\") \n",
    "print(\"=\" * 150)\n",
    "print(\"Example join output:\")\n",
    "cust_bought_mouse.show(3, truncate=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Left Join\n",
    "\n",
    "Suppose we wish to keep all customers, but create a flag as to whether they bought a mouse or not, through a Y or N. We can do this via a left join, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+--------+--------------+------------+\n",
      "|cust_id|firstName|lastName|product_bought|bought_mouse|\n",
      "+-------+---------+--------+--------------+------------+\n",
      "|10340  |Denis    |Jones   |null          |N           |\n",
      "|10683  |Emily    |Evans   |null          |N           |\n",
      "|10702  |Gareth   |Pearson |null          |N           |\n",
      "+-------+---------+--------+--------------+------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "======================================================================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 32:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Customer who DID NOT buy a mouse: 129489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# import\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "# let's say we want to create a dataframe that has customer names of people who purchased a \"mouse\" from the two dataframes \n",
    "mouse_txns = (\n",
    "    txns_df # source data\n",
    "    .filter(upper(txns_df[\"Product\"]) == \"MOUSE\") # apply where clause to restrict rows \n",
    "    .select(\"customerID\", \"product\") # select only the columns of interest \n",
    ").distinct() # should ensure we only get back distinct records of customer id & mouse purchases \n",
    "\n",
    "\n",
    "lag_mouse_purchase = (\n",
    "    customer_df.join(\n",
    "        mouse_txns,\n",
    "        customer_df[\"customerID\"] == mouse_txns[\"customerID\"],\n",
    "        \"left\"\n",
    "    ).select(\n",
    "        customer_df[\"customerID\"].alias(\"cust_id\"), # selects customer_id from df1, renames it to `cust_id` via alias \n",
    "        customer_df[\"firstName\"], customer_df[\"lastName\"],\n",
    "        mouse_txns[\"Product\"].alias(\"product_bought\")\n",
    "    )\n",
    ").distinct() # ensure no duplicate customers\n",
    "\n",
    "# create a Y / N marker for if mouse was purchased \n",
    "mouse_marker = lag_mouse_purchase.withColumn(\n",
    "        \"bought_mouse\", # name of new column \n",
    "        when(lag_mouse_purchase[\"product_bought\"].isNotNull(), 'Y') # note, you can chain multiple \"when\" here via dot notation \n",
    "        .otherwise(\"N\") # case when style condition to determine column value\n",
    ")\n",
    "\n",
    "# show example:\n",
    "mouse_marker.show(3, truncate=False)\n",
    "\n",
    "print(\"=\" * 150) \n",
    "# filter to just NO mouse purchased \n",
    "no_mouse = mouse_marker.filter(\n",
    "    mouse_marker[\"bought_mouse\"] == \"N\"\n",
    ").count() \n",
    "\n",
    "print(f\"No. of Customer who DID NOT buy a mouse: {no_mouse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Join\n",
    "\n",
    "Let's take a look at a cross join. Also known as a cartesian product join. This is where each combination of rows from two dataframes is returned. Often, this could be used in things like creating a date map with a list of products, before then wanting to join to this results from another source. Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      Date|\n",
      "+----------+\n",
      "|2024-01-01|\n",
      "|2024-02-01|\n",
      "|2024-03-01|\n",
      "|2024-04-01|\n",
      "+----------+\n",
      "\n",
      "======================================================================================================================================================\n",
      "+-------+\n",
      "|Product|\n",
      "+-------+\n",
      "|    Hat|\n",
      "|   Belt|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import \n",
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "# Create a sample DataFrame with string dates\n",
    "data1 = [(\"2024-01-01\",), (\"2024-02-01\",), (\"2024-03-01\",), (\"2024-04-01\",)]\n",
    "columns1 = [\"Date\"]\n",
    "date_df = (spark.createDataFrame(data1, columns1)).withColumn(\"Date\", to_date(col(\"Date\"), \"yyyy-MM-dd\"))\n",
    "date_df.show() \n",
    "\n",
    "print(\"=\" * 150) \n",
    "\n",
    "# create simple products dataframe \n",
    "data2 = [(\"Hat\",), (\"Belt\",)]\n",
    "columns2 = [\"Product\"] \n",
    "product_df = spark.createDataFrame(data2, columns2) \n",
    "product_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|      Date|Product|\n",
      "+----------+-------+\n",
      "|2024-01-01|    Hat|\n",
      "|2024-01-01|   Belt|\n",
      "|2024-02-01|    Hat|\n",
      "|2024-02-01|   Belt|\n",
      "|2024-03-01|    Hat|\n",
      "|2024-03-01|   Belt|\n",
      "|2024-04-01|    Hat|\n",
      "|2024-04-01|   Belt|\n",
      "+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cross_join_df = (\n",
    "    date_df.crossJoin(product_df)\n",
    ").orderBy(col(\"Date\"))\n",
    "\n",
    "cross_join_df.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full Outer Join\n",
    "\n",
    "This is where we can join tables, but keep all records! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+\n",
      "| Name| Age|Score|\n",
      "+-----+----+-----+\n",
      "|  Dan|  31|   98|\n",
      "|Sally|null|  100|\n",
      "|Simon|  30| null|\n",
      "|  Tim|  40|   92|\n",
      "+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import coalesce, col\n",
    "\n",
    "# example dataframe 1 \n",
    "test1 = [(\"Dan\", 31), (\"Simon\", 30), (\"Tim\", 40)]\n",
    "cols1 = [\"Name\", \"Age\"]\n",
    "df1 = spark.createDataFrame(test1, cols1) \n",
    "#df1.show() \n",
    "\n",
    "# example dataframe 2 \n",
    "test2 = [(\"Dan\", 98), (\"Sally\", 100), (\"Tim\", 92)]\n",
    "cols2 = [\"Name\", \"Score\"]\n",
    "df2 = spark.createDataFrame(test2, cols2) \n",
    "#df2.show() \n",
    "\n",
    "# Full Outer Join - should see Simon have no score, & Sally have no Age \n",
    "full_join_df = (\n",
    "    df1.join(\n",
    "        df2, \n",
    "        df1[\"Name\"] == df2[\"Name\"],\n",
    "        \"full_outer\" # specify the full condition \n",
    "    ).select(\n",
    "        df1[\"Name\"].alias(\"Name1\"),\n",
    "        df2[\"Name\"].alias(\"Name2\"),\n",
    "        df1[\"Age\"], df2[\"Score\"]\n",
    "    ).withColumn(\n",
    "        \"Name\",\n",
    "        coalesce(col('Name1'), col('Name2'))\n",
    "    ).select(\n",
    "        \"Name\", \"Age\", \"Score\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# execute & show output \n",
    "full_join_df.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Left Semi Join\n",
    "\n",
    "This is where we can use Table B, to filter Table A and retain only records from Table A that are also in Table B "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id| name|\n",
      "+---+-----+\n",
      "|  1|Alice|\n",
      "|  2|  Bob|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrames for Users and Purchases\n",
    "data_users = [(1, \"Alice\"), (2, \"Bob\"), (3, \"Charlie\"), (4, \"David\")]\n",
    "data_purchases = [(1, \"Book\"), (2, \"Pen\"), (5, \"Notebook\")]\n",
    "\n",
    "columns_users = [\"id\", \"name\"]\n",
    "columns_purchases = [\"user_id\", \"item\"]\n",
    "\n",
    "df_users = spark.createDataFrame(data_users, columns_users)\n",
    "df_purchases = spark.createDataFrame(data_purchases, columns_purchases)\n",
    "\n",
    "# Perform Left Semi Join\n",
    "df_purchasers = df_users.join(df_purchases, df_users.id == df_purchases.user_id, \"left_semi\")\n",
    "\n",
    "# Show the result - IDs 1 & 2 should return, as they are in the `df_purchases` dataframe, but 3 & 4 are not \n",
    "df_purchasers.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Left Anti Join \n",
    "\n",
    "This is where we use Table B, to filter Table A and only return rows from Table A that ARE NOT in Table B "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|   name|\n",
      "+---+-------+\n",
      "|  3|Charlie|\n",
      "|  4|  David|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrames for Users and Purchases\n",
    "data_users = [(1, \"Alice\"), (2, \"Bob\"), (3, \"Charlie\"), (4, \"David\")]\n",
    "data_purchases = [(1, \"Book\"), (2, \"Pen\"), (5, \"Notebook\")]\n",
    "\n",
    "columns_users = [\"id\", \"name\"]\n",
    "columns_purchases = [\"user_id\", \"item\"]\n",
    "\n",
    "df_users = spark.createDataFrame(data_users, columns_users)\n",
    "df_purchases = spark.createDataFrame(data_purchases, columns_purchases)\n",
    "\n",
    "# Perform Left Anti Join\n",
    "df_non_purchasers = (df_users.join(\n",
    "    df_purchases, \n",
    "    df_users.id == df_purchases.user_id, \n",
    "    \"left_anti\")\n",
    ")\n",
    "\n",
    "# Show the result - this time, IDs 3 & 4 should appear, as they are not in `df_purchases` \n",
    "df_non_purchasers.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Practices on Joins\n",
    "\n",
    "1. `Understand Your Data`<br>\n",
    "**Pre-Inspect Data:** Familiarize yourself with the data, its size, distribution, and the columns you plan to join on. Understanding the nature of your data can help in choosing the most efficient join type and strategy.<br>\n",
    "**Check for Duplicates:** Ensure that the keys you're joining on don't have unexpected duplicates, which can cause inflated results and performance issues.<br>\n",
    "\n",
    "2. `Optimize Data Size`<br>\n",
    "**Filter Early:** Apply filters to reduce the size of DataFrames before joining. Smaller DataFrames require less time and memory to join.<br>\n",
    "**Select Necessary Columns:** Only select the columns needed for analysis before joining to reduce data shuffle.<br>\n",
    "\n",
    "3. `Manage Skewness`<br>\n",
    "**Detect Skew:** Identify if your data is skewed, meaning some keys have significantly more data than others. Skewness can lead to unequal distribution of data and can severely impact join performance.<br>\n",
    "**Handle Skew:** If skew is detected, consider using techniques like salting or broadcasting smaller tables to minimize its impact.<br>\n",
    "\n",
    "4. `Use Broadcast Joins for Small Tables`<br>\n",
    "**Broadcast Small DataFrames:** If one of your DataFrames is small enough, use a broadcast join to send it to all nodes. This avoids shuffling large DataFrames across the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 18.1 - Broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcasting example \n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "df_large.join(broadcast(df_small), join_condition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. `Choose the Right Join Type`<br>\n",
    "**Appropriate Join Type:** Use the join type that suits your data and requirements. For instance, if you only need matching rows, consider using an inner join; if you need to retain all rows from one DataFrame, consider using an outer join.<br>\n",
    "\n",
    "6. `Partitioning and Clustering`<br>\n",
    "**Effective Partitioning:** Ensure your DataFrames are partitioned effectively before joining. Partitioning on the join key can reduce shuffling.\n",
    "Clustering: If possible, cluster your data on the join key. Clustering can significantly speed up join operations as related data is physically stored together.<br>\n",
    "\n",
    "7. `Monitor and Tune`<br>\n",
    "**Examine Execution Plans:** Use .explain() to understand the physical and logical plan Spark is using to execute your join. Look for opportunities to reduce shuffles and stages.<br>\n",
    "**Tune Spark Configuration:** Adjust Spark configurations like spark.sql.shuffle.partitions to better suit your job's requirements and available cluster resources.<br>\n",
    "\n",
    "8. `Avoid Cartesian Joins`<br>\n",
    "**Be Cautious with Cross Joins:** Only use cross joins when absolutely necessary, as they can produce an extensive number of rows and significantly degrade performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop() # closes spark session "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "#### 19. Aggregations & Summaries\n",
    "\n",
    "Here, we take a look at how we can perform Group Bys, creating aggregated metrics such as counts, sums, min, max & mean<br>\n",
    "\n",
    "It also covers casting, rounding and aliases for the output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "24/05/31 15:28:31 WARN Utils: Your hostname, DCollins-Laptop1 resolves to a loopback address: 127.0.1.1; using 172.26.39.146 instead (on interface eth0)\n",
      "24/05/31 15:28:31 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/31 15:28:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+---------+------+-----+----------+\n",
      "|customerID|transaction_TS     |Product  |volume|Price|txn_amount|\n",
      "+----------+-------------------+---------+------+-----+----------+\n",
      "|142387    |2023-10-15 15:42:50|Projector|6.0   |300.0|1800.0    |\n",
      "|126774    |2023-10-15 23:40:19|Mouse    |1.0   |8.0  |8.0       |\n",
      "|26995     |2023-10-15 20:43:32|Projector|3.0   |300.0|900.0     |\n",
      "+----------+-------------------+---------+------+-----+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import * \n",
    "from pyspark.sql.types import DecimalType\n",
    "\n",
    "# create session \n",
    "spark = SparkSession.builder\\\n",
    "            .appName(\"my_local_spark_session\")\\\n",
    "            .master(\"local\")\\\n",
    "            .getOrCreate() \n",
    "sc = spark.sparkContext \n",
    "\n",
    "# read in some data to a dataframe \n",
    "# transactions data \n",
    "txns_Path = \"./data/dummy_txns.csv\"\n",
    "txns_df = (spark.read\n",
    "    .option(\"delimiter\", \",\") # sets the delimiter to `,`\n",
    "    .option(\"header\", \"true\") # informs pyspark that row 1 should be treated as the column headings of the data \n",
    "    .option(\"inferSchema\", \"true\") # lets spark infer the schema of the data itself, rather than us expliitly creating a schema\n",
    "    .csv(txns_Path) \n",
    ")\n",
    "txns_df.show(3, truncate=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at doing Group By, alongside the max, min & avg transaction amount, for each product!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:==========================================================(1 + 0) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+------------------+-------------------+-------------------+-------------------+-------------------+\n",
      "|Product              |no_of_transactions|sum_of_transactions|min_transaction_amt|max_transaction_amt|avg_transaction_amt|\n",
      "+---------------------+------------------+-------------------+-------------------+-------------------+-------------------+\n",
      "|WiFi Range Extender  |11097             |1169790.00         |30.00              |180.00             |105.41             |\n",
      "|USB Flash Drive 16gb |11139             |156807.00          |3.99               |23.94              |14.08              |\n",
      "|Tablet               |11075             |4409560.00         |115.00             |690.00             |398.15             |\n",
      "|Projector            |11213             |11743500.00        |300.00             |1800.00            |1047.31            |\n",
      "|Printer              |10924             |2679390.00         |70.00              |420.00             |245.28             |\n",
      "|Office Chair Standard|11035             |6187840.00         |160.00             |960.00             |560.75             |\n",
      "|Office Chair Premium |11079             |9594500.00         |250.00             |1500.00            |866.01             |\n",
      "|Mouse                |10962             |304976.00          |8.00               |48.00              |27.82              |\n",
      "|Monitor              |11121             |4644360.00         |120.00             |720.00             |417.62             |\n",
      "|Laptop Stand         |11296             |516963.03          |12.99              |77.94              |45.77              |\n",
      "|Laptop Bag           |11125             |2140545.00         |55.00              |330.00             |192.41             |\n",
      "|Laptop               |11147             |15664408.38        |399.99             |2399.94            |1405.26            |\n",
      "|Keyboard             |11199             |1369235.00         |35.00              |210.00             |122.26             |\n",
      "|HDMI Cable           |10988             |577538.92          |14.98              |89.88              |52.56              |\n",
      "|Extension Cable      |11227             |196770.67          |4.99               |29.94              |17.53              |\n",
      "|Docking Station      |11174             |2746520.00         |70.00              |420.00             |245.80             |\n",
      "|Desktop              |11085             |23437409.37        |599.99             |3599.94            |2114.34            |\n",
      "|Desk                 |11114             |15385600.00        |400.00             |2400.00            |1384.34            |\n",
      "+---------------------+------------------+-------------------+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "product_stats_df = (\n",
    "    txns_df # specify source data frame \n",
    "    .select(\"Product\", \"volume\", \"Price\", \"txn_amount\") # select initial columns\n",
    "    .groupBy(\"Product\") # set a group by on `product` \n",
    "    .agg(\n",
    "        count(col(\"txn_amount\")).alias(\"no_of_transactions\"), # create an agg for metrics & round to 2 decimal places,cast as decimal type\n",
    "        round(sum(col(\"txn_amount\")), 2).cast(DecimalType(10, 2)).alias(\"sum_of_transactions\"), # total sum\n",
    "        round(min(col(\"txn_amount\")), 2).cast(DecimalType(10, 2)).alias(\"min_transaction_amt\"), # min transaction amount\n",
    "        round(max(col(\"txn_amount\")), 2).cast(DecimalType(10, 2)).alias(\"max_transaction_amt\"), # max amount \n",
    "        round(avg(col(\"txn_amount\")), 2).cast(DecimalType(10, 2)).alias(\"avg_transaction_amt\")  # avg amount \n",
    "    )\n",
    "    .select(\n",
    "        \"Product\", \"no_of_transactions\", \"sum_of_transactions\", \n",
    "        \"min_transaction_amt\", \"max_transaction_amt\", \"avg_transaction_amt\"\n",
    "    )\n",
    "    .orderBy(col(\"Product\").desc()) # order by `product` descending \n",
    ")\n",
    "\n",
    "product_stats_df.show(truncate=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 20. Window Functions\n",
    "\n",
    "Here, we take a look at window functions. These offer a powerful way to perform calculations, such as `lead` & `lag` or `row_number` or `rank`<br>\n",
    "These can also be achieved through self-joins, but window functions require much less code\n",
    "\n",
    "\n",
    "Using the transaction dataframe, let's look at a window function, to find the *most* recent transaction per customer, for the date in question\n",
    "\n",
    "First, let's confirm that we do indeed have some customers with more than 1 transaction in a day ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|customerID|no_of_txns|\n",
      "+----------+----------+\n",
      "|114542    |9         |\n",
      "|137076    |9         |\n",
      "|108567    |9         |\n",
      "|94604     |8         |\n",
      "|33420     |8         |\n",
      "+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "multi_txns = (\n",
    "    txns_df\n",
    "        .select(\"customerID\", \"transaction_TS\")\n",
    "        .groupBy(\"customerID\")\n",
    "        .agg(\n",
    "            countDistinct(\"transaction_TS\").alias(\"no_of_txns\")\n",
    "        )\n",
    "        .select(\"customerID\", \"no_of_txns\")\n",
    "        .orderBy(col(\"no_of_txns\").desc())\n",
    ")\n",
    "\n",
    "# show top 5 wos \n",
    "multi_txns.show(5, truncate=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "great!<br>\n",
    "we can see from above that we have instances of customers purchasing more than once during the day. Let's use a window function, to rank the purchases by the timestamp they happened, and keep the most recent transaction per customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the window method \n",
    "from pyspark.sql.window import Window "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+--------------------+------+-----+----------+--------+\n",
      "|customerID|transaction_TS     |Product             |volume|Price|txn_amount|txn_rank|\n",
      "+----------+-------------------+--------------------+------+-----+----------+--------+\n",
      "|10000     |2023-10-15 16:40:44|Office Chair Premium|2.0   |250.0|500.0     |1       |\n",
      "|10004     |2023-10-15 14:22:21|USB Flash Drive 16gb|3.0   |3.99 |11.97     |1       |\n",
      "|10005     |2023-10-15 02:34:08|Keyboard            |5.0   |35.0 |175.0     |1       |\n",
      "|10006     |2023-10-15 18:24:22|Extension Cable     |6.0   |4.99 |29.94     |1       |\n",
      "|10007     |2023-10-15 09:29:21|USB Flash Drive 16gb|1.0   |3.99 |3.99      |1       |\n",
      "+----------+-------------------+--------------------+------+-----+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a \"window\" definition. This is the PARTITION BY & ORDER BY within SQL \n",
    "windowDef = (\n",
    "    Window.partitionBy(\"customerID\").orderBy(col(\"transaction_TS\").desc())\n",
    ")\n",
    "\n",
    "# apply the window function within row_number() \n",
    "cust_txns_ranked = (\n",
    "    txns_df\n",
    "        .withColumn(     # new column\n",
    "            \"txn_rank\",  # define it's name \n",
    "            row_number().over(windowDef) # specify it's value as using row_number(), by desc txn timestamp, so most recent has value = 1\n",
    "        )\n",
    ")\n",
    "# find latest txn per customer with filter to `txn_rank` = 1 \n",
    "cust_last_txn = (\n",
    "    cust_txns_ranked\n",
    "        .filter(col(\"txn_rank\") == 1)\n",
    "        .drop(\"cust_txns_ranked\")\n",
    ")\n",
    "\n",
    "# no show the results \n",
    "cust_last_txn.show(5, truncate=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prove the case, take customerID `10000` from above, and pull their records from the original dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+--------------------+------+-----+----------+\n",
      "|customerID|transaction_TS     |Product             |volume|Price|txn_amount|\n",
      "+----------+-------------------+--------------------+------+-----+----------+\n",
      "|10000     |2023-10-15 01:13:36|Printer             |3.0   |70.0 |210.0     |\n",
      "|10000     |2023-10-15 16:40:44|Office Chair Premium|2.0   |250.0|500.0     |\n",
      "+----------+-------------------+--------------------+------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "checkout = (\n",
    "    txns_df\n",
    "        .filter(col(\"customerID\") == 10000)\n",
    ")\n",
    "checkout.show(truncate=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we can see from above, the ROW_NUMBER & filter to latest transaction has worked as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop() # close spark session "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "\n",
    "#### 21. Using Spark SQL within PySpark framework\n",
    "\n",
    "We can make use of Spark SQL from within the PySpark framework, by using the `.sql()` method, which can run queries\n",
    "\n",
    "Let's take a look at a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+---------+------+-----+----------+\n",
      "|customerID|transaction_TS     |Product  |volume|Price|txn_amount|\n",
      "+----------+-------------------+---------+------+-----+----------+\n",
      "|142387    |2023-10-15 15:42:50|Projector|6.0   |300.0|1800.0    |\n",
      "|126774    |2023-10-15 23:40:19|Mouse    |1.0   |8.0  |8.0       |\n",
      "|26995     |2023-10-15 20:43:32|Projector|3.0   |300.0|900.0     |\n",
      "+----------+-------------------+---------+------+-----+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import \n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# create session \n",
    "spark = SparkSession.builder\\\n",
    "            .appName(\"my_local_spark_session\")\\\n",
    "            .master(\"local\")\\\n",
    "            .getOrCreate() \n",
    "sc = spark.sparkContext \n",
    "\n",
    "# read in some data to a dataframe \n",
    "# transactions data \n",
    "txns_Path = \"./data/dummy_txns.csv\"\n",
    "txns_df = (spark.read\n",
    "    .option(\"delimiter\", \",\") # sets the delimiter to `,`\n",
    "    .option(\"header\", \"true\") # informs pyspark that row 1 should be treated as the column headings of the data \n",
    "    .option(\"inferSchema\", \"true\") # lets spark infer the schema of the data itself, rather than us expliitly creating a schema\n",
    "    .csv(txns_Path) \n",
    ")\n",
    "txns_df.show(3, truncate=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in order to use SQL on a dataframe in PySpark, you need to register a temporary view. This will allow for SQL to be performed upon the dataframe. \n",
    "\n",
    "***(Note, some installations of PySpark can allow for direct SQL on dataframes without this)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|PRODUCT             |NUM_SOLD|\n",
      "+--------------------+--------+\n",
      "|Laptop Stand        |39797.0 |\n",
      "|Extension Cable     |39433.0 |\n",
      "|USB Flash Drive 16gb|39300.0 |\n",
      "|Docking Station     |39236.0 |\n",
      "|Laptop              |39162.0 |\n",
      "+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create temp view \n",
    "txns_df.createOrReplaceTempView(\"txns_data\") # you name the view from which you wish your sql query to reference \n",
    "\n",
    "simple_query = \"\"\"\n",
    "SELECT\n",
    "    PRODUCT, \n",
    "    SUM(VOLUME) AS NUM_SOLD\n",
    "FROM txns_data\n",
    "GROUP BY PRODUCT\n",
    "ORDER BY NUM_SOLD DESC \n",
    "\"\"\"\n",
    "\n",
    "test_sql_df = spark.sql(simple_query) # this executes the Spark SQL query above\n",
    "\n",
    "test_sql_df.show(5, truncate=False) # show results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, there is a change here. Introduced in `Spark 3.4`, you can directly perform queries on the dataframe itself, without needing to create a temporary view first. <br>\n",
    "\n",
    "We can use parameters to specify the dataframe to target in the query!\n",
    "\n",
    "let's take a look ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|PRODUCT             |NUM_SOLD|\n",
      "+--------------------+--------+\n",
      "|Laptop Stand        |39797.0 |\n",
      "|Extension Cable     |39433.0 |\n",
      "|USB Flash Drive 16gb|39300.0 |\n",
      "|Docking Station     |39236.0 |\n",
      "|Laptop              |39162.0 |\n",
      "+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "another_sql_df = (\n",
    "    spark.sql(\n",
    "        \"SELECT PRODUCT, SUM(VOLUME) AS NUM_SOLD FROM {source_df} GROUP BY PRODUCT ORDER BY NUM_SOLD DESC\", # the query\n",
    "        source_df=txns_df    # specify the source dataframe (without creating a view in the session) \n",
    "    )\n",
    ")\n",
    "another_sql_df.show(5, truncate=False) # show some results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop() # close the spark session "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------\n",
    "\n",
    "#### 22. Writing Data Out\n",
    "\n",
    "You can write data out to file storage in a variety of formats, for example, let's Read a CSV, and write it back out as Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+---------+------+-----+----------+\n",
      "|customerID|transaction_TS     |Product  |volume|Price|txn_amount|\n",
      "+----------+-------------------+---------+------+-----+----------+\n",
      "|142387    |2023-10-15 15:42:50|Projector|6.0   |300.0|1800.0    |\n",
      "|126774    |2023-10-15 23:40:19|Mouse    |1.0   |8.0  |8.0       |\n",
      "|26995     |2023-10-15 20:43:32|Projector|3.0   |300.0|900.0     |\n",
      "+----------+-------------------+---------+------+-----+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# import \n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# create session \n",
    "spark = SparkSession.builder\\\n",
    "            .appName(\"my_local_spark_session\")\\\n",
    "            .master(\"local\")\\\n",
    "            .getOrCreate() \n",
    "sc = spark.sparkContext \n",
    "\n",
    "# read in some data to a dataframe \n",
    "# transactions data \n",
    "txns_Path = \"./data/dummy_txns.csv\"\n",
    "txns_df = (spark.read\n",
    "    .option(\"delimiter\", \",\") # sets the delimiter to `,`\n",
    "    .option(\"header\", \"true\") # informs pyspark that row 1 should be treated as the column headings of the data \n",
    "    .option(\"inferSchema\", \"true\") # lets spark infer the schema of the data itself, rather than us expliitly creating a schema\n",
    "    .csv(txns_Path) \n",
    ")\n",
    "txns_df.show(3, truncate=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# write the `txns_df` out as parquet to the same file location as the CSV file \n",
    "output_path = \"./data/parquet_dummy_txns\"\n",
    "txns_df.write.parquet(output_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+---------+------+-----+----------+\n",
      "|customerID|     transaction_TS|  Product|volume|Price|txn_amount|\n",
      "+----------+-------------------+---------+------+-----+----------+\n",
      "|    142387|2023-10-15 15:42:50|Projector|   6.0|300.0|    1800.0|\n",
      "|    126774|2023-10-15 23:40:19|    Mouse|   1.0|  8.0|       8.0|\n",
      "|     26995|2023-10-15 20:43:32|Projector|   3.0|300.0|     900.0|\n",
      "|    118641|2023-10-15 02:45:11|Projector|   3.0|300.0|     900.0|\n",
      "|    112835|2023-10-15 17:39:18|Projector|   1.0|300.0|     300.0|\n",
      "+----------+-------------------+---------+------+-----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read back in as test\n",
    "df_parquet = spark.read.parquet(output_path)\n",
    "\n",
    "df_parquet.show(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop() # close spark session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "\n",
    "#### 23. Delta Tables & Partitioning\n",
    "\n",
    "A key component of working with PySpark is how you save the outputs of your transformations. A powerful open-source format known as `delta` is a great option for a table-format of your datalakes & lakehouses. Some key reasons:\n",
    "\n",
    "* Delta is an efficient, compressed format (utilises Parquet) that stores data as files, and stores relevant metadata about the files, so that you can have a \"table\" over files. This links back to Hive & HDFS from the Hadoop ecosystem many years back (but still in use today).\n",
    "\n",
    "* Delta supports ACID properties, even though it is not a database like we are traditionally used to. This means we can perform actions like UPSERT on data, rather than just overwriting whole tables or adding new partitions, which is functionality that has existed for a while. \n",
    "\n",
    "* This also means that delta supports the concept of data versioning. The metadata log, keeps track of the changes, meaning you can move between historic versions of the data in the delta table when reading files etc. It also means that your datalake can support things like schema changes to tables, without needing to overwrite all existing files that hold the data etc.\n",
    "\n",
    "Let's take a look at some examples of using Delta with PySpark (v 3.4):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you don't already have the Delta-Core installed, you'll need these in your environment where PySpark is installed \n",
    "# Delta-Lake 2.4.x is the compatible version with Apache Spark 3.4.x\n",
    "\n",
    "#!pip install pip install delta-spark==2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from delta import *\n",
    "from pyspark.sql.types import *\n",
    "from delta.tables import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "#  Create a spark session with Delta\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"DeltaTutorial\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "# Create spark context\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we now have a local Spark Session which has the relevant Delta configuration as well. Let's take a look at some examples of how to use Delta with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---------+-------------+---+\n",
      "|firstname| lastname|    house|     location|age|\n",
      "+---------+---------+---------+-------------+---+\n",
      "|   Robert|Baratheon|Baratheon|   Storms End| 48|\n",
      "|   Eddard|    Stark|    Stark|   Winterfell| 46|\n",
      "|    Jamie|Lannister|Lannister|Casterly Rock| 29|\n",
      "+---------+---------+---------+-------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create some example data \n",
    "data = [(\"Robert\", \"Baratheon\", \"Baratheon\", \"Storms End\", 48),\n",
    "        (\"Eddard\", \"Stark\", \"Stark\", \"Winterfell\", 46),\n",
    "        (\"Jamie\", \"Lannister\", \"Lannister\", \"Casterly Rock\", 29)\n",
    "        ]\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\", StringType(), True),\n",
    "    StructField(\"lastname\", StringType(), True),\n",
    "    StructField(\"house\", StringType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "sample_dataframe = spark.createDataFrame(data=data, schema=schema)\n",
    "\n",
    "sample_dataframe.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write out our first delta table! This will be to a local location in the `./data` folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# write out data to delta format\n",
    "sample_dataframe.write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"delta\")\\\n",
    "    .save(\"./data/got_delta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at reading that delta table back in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---------+-------------+---+\n",
      "|firstname| lastname|    house|     location|age|\n",
      "+---------+---------+---------+-------------+---+\n",
      "|    Jamie|Lannister|Lannister|Casterly Rock| 29|\n",
      "|   Robert|Baratheon|Baratheon|   Storms End| 48|\n",
      "|   Eddard|    Stark|    Stark|   Winterfell| 46|\n",
      "+---------+---------+---------+-------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "got_df = spark.read.format(\"delta\").load(\"./data/got_delta\")\n",
    "got_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overwrite the entire table ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# notice we are changing the ages \n",
    "data = [(\"Robert\", \"Baratheon\", \"Baratheon\", \"Storms End\", 49),\n",
    "        (\"Eddard\", \"Stark\", \"Stark\", \"Winterfell\", 47),\n",
    "        (\"Jamie\", \"Lannister\", \"Lannister\", \"Casterly Rock\", 30)\n",
    "        ]\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\", StringType(), True),\n",
    "    StructField(\"lastname\", StringType(), True),\n",
    "    StructField(\"house\", StringType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True)\n",
    "])\n",
    "sample_dataframe = spark.createDataFrame(data=data, schema=schema)\n",
    "(sample_dataframe.write\n",
    "    .mode(saveMode=\"overwrite\") # specify to overwrite \n",
    "    .format(\"delta\")            # delta format\n",
    "    .save(\"./data/got_delta\")   # same location for the files as before, so they are overwritten \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read that back in again, and notice the new ages!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---------+-------------+---+\n",
      "|firstname| lastname|    house|     location|age|\n",
      "+---------+---------+---------+-------------+---+\n",
      "|    Jamie|Lannister|Lannister|Casterly Rock| 30|\n",
      "|   Robert|Baratheon|Baratheon|   Storms End| 49|\n",
      "|   Eddard|    Stark|    Stark|   Winterfell| 47|\n",
      "+---------+---------+---------+-------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "got_df = spark.read.format(\"delta\").load(\"./data/got_delta\")\n",
    "got_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look at upserting a Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---------+-------------+---+\n",
      "|firstname| lastname|    house|     location|age|\n",
      "+---------+---------+---------+-------------+---+\n",
      "|    Jamie|Lannister|Lannister|Casterly Rock| 30|\n",
      "|   Robert|Baratheon|Baratheon|   Storms End| 49|\n",
      "|   Eddard|    Stark|    Stark|   Winterfell| 47|\n",
      "+---------+---------+---------+-------------+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---------+-------------+---+\n",
      "|firstname| lastname|    house|     location|age|\n",
      "+---------+---------+---------+-------------+---+\n",
      "|   Gendry|Baratheon|Baratheon|Kings Landing| 19|\n",
      "|    Jamie|Lannister|Lannister|Casterly Rock| 36|\n",
      "|      Jon|     Snow|    Stark|   Winterfell| 21|\n",
      "|   Robert|Baratheon|Baratheon|   Storms End| 49|\n",
      "|   Eddard|    Stark|    Stark|   Winterfell| 47|\n",
      "+---------+---------+---------+-------------+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Import \n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Upsert Data\n",
    "# delta table path\n",
    "deltaTable = DeltaTable.forPath(spark, \"./data/got_delta\")\n",
    "deltaTable.toDF().show()\n",
    "\n",
    "# define new data\n",
    "data = [(\"Gendry\", \"Baratheon\", \"Baratheon\", \"Kings Landing\", 19),\n",
    "        (\"Jon\", \"Snow\", \"Stark\", \"Winterfell\", 21),\n",
    "        (\"Jamie\", \"Lannister\", \"Lannister\", \"Casterly Rock\", 36)\n",
    "        ]\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\", StringType(), True),\n",
    "    StructField(\"lastname\", StringType(), True),\n",
    "    StructField(\"house\", StringType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "newData = spark.createDataFrame(data=data, schema=schema)\n",
    "\n",
    "(\n",
    "    deltaTable\n",
    "        .alias(\"oldData\") # alias the current dataframe as old table\n",
    "        .merge(\n",
    "            newData.alias(\"newData\"),\"oldData.firstname = newData.firstname\" # specify what dataframe to merge in & how\n",
    "        )\n",
    "        .whenMatchedUpdate( # specify what to do on when olData.firtname = newData.firstname matches. You use UPDATE\n",
    "            set={\n",
    "                \"firstname\": col(\"newData.firstname\"), \n",
    "                \"lastname\": col(\"newData.lastname\"), \n",
    "                \"house\": col(\"newData.house\"),\n",
    "                \"location\": col(\"newData.location\"), \n",
    "                \"age\": col(\"newData.age\")\n",
    "            }\n",
    "        )\n",
    "    .whenNotMatchedInsert( # specify what to do on when olData.firtname = newData.firstname do NOT match. You use INSERT\n",
    "        values={\n",
    "            \"firstname\": col(\"newData.firstname\"), \n",
    "            \"lastname\": col(\"newData.lastname\"), \n",
    "            \"house\": col(\"newData.house\"),\n",
    "            \"location\": col(\"newData.location\"), \n",
    "            \"age\": col(\"newData.age\")\n",
    "        }\n",
    "    )\n",
    "    .execute()\n",
    ")\n",
    "\n",
    "# look at the new dataframe ...\n",
    "deltaTable.toDF().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as you can see from above, Jamie Lannister was updated because he already existed, and Jon & Gendry were inserted, as they did not exist before\n",
    "\n",
    "So, below, if we read the delta table into a new spark dataframe, notice the new updates above into that dataframe!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---------+-------------+---+\n",
      "|firstname| lastname|    house|     location|age|\n",
      "+---------+---------+---------+-------------+---+\n",
      "|   Gendry|Baratheon|Baratheon|Kings Landing| 19|\n",
      "|    Jamie|Lannister|Lannister|Casterly Rock| 36|\n",
      "|      Jon|     Snow|    Stark|   Winterfell| 21|\n",
      "|   Robert|Baratheon|Baratheon|   Storms End| 49|\n",
      "|   Eddard|    Stark|    Stark|   Winterfell| 47|\n",
      "+---------+---------+---------+-------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "updated_got_df = spark.read.format(\"delta\").load(\"./data/got_delta\")\n",
    "updated_got_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let's say we want to delete a record?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---------+-------------+---+\n",
      "|firstname| lastname|    house|     location|age|\n",
      "+---------+---------+---------+-------------+---+\n",
      "|   Robert|Baratheon|Baratheon|   Storms End| 49|\n",
      "|   Eddard|    Stark|    Stark|   Winterfell| 47|\n",
      "|    Jamie|Lannister|Lannister|Casterly Rock| 36|\n",
      "|      Jon|     Snow|    Stark|   Winterfell| 21|\n",
      "+---------+---------+---------+-------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's remove the Gendry record \n",
    "# delta table path\n",
    "deltaTable = DeltaTable.forPath(spark, \"./data/got_delta\")\n",
    "#deltaTable.toDF().show()\n",
    "\n",
    "deltaTable.delete(condition=expr(\"firstname == 'Gendry'\"))\n",
    "\n",
    "deltaTable.toDF().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so, if we repeat the steps above, reading in the delta table to a new spark dataframe, we'll see gendry has been deleted!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---------+-------------+---+\n",
      "|firstname| lastname|    house|     location|age|\n",
      "+---------+---------+---------+-------------+---+\n",
      "|   Robert|Baratheon|Baratheon|   Storms End| 49|\n",
      "|   Eddard|    Stark|    Stark|   Winterfell| 47|\n",
      "|    Jamie|Lannister|Lannister|Casterly Rock| 36|\n",
      "|      Jon|     Snow|    Stark|   Winterfell| 21|\n",
      "+---------+---------+---------+-------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "updated_got_df_again = spark.read.format(\"delta\").load(\"./data/got_delta\")\n",
    "updated_got_df_again.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, above, we've looked at how we can perform transactions on delta tables with PySpark\n",
    "\n",
    "Finally, let's take a quick look at the version history of delta, and how we can read that into dataframes ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---------+-------------+---+\n",
      "|firstname| lastname|    house|     location|age|\n",
      "+---------+---------+---------+-------------+---+\n",
      "|    Jamie|Lannister|Lannister|Casterly Rock| 29|\n",
      "|   Robert|Baratheon|Baratheon|   Storms End| 48|\n",
      "|   Eddard|    Stark|    Stark|   Winterfell| 46|\n",
      "+---------+---------+---------+-------------+---+\n",
      "\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---------+-------------+---+\n",
      "|firstname| lastname|    house|     location|age|\n",
      "+---------+---------+---------+-------------+---+\n",
      "|    Jamie|Lannister|Lannister|Casterly Rock| 30|\n",
      "|   Robert|Baratheon|Baratheon|   Storms End| 49|\n",
      "|   Eddard|    Stark|    Stark|   Winterfell| 47|\n",
      "+---------+---------+---------+-------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_versionzero = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(\"./data/got_delta\")\n",
    "df_versionzero.show() # uses version zero of the delta table (aka the first version of the table)\n",
    "\n",
    "print(\"=\" * 100) \n",
    "\n",
    "df_versionzone = spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(\"./data/got_delta\")\n",
    "df_versionzone.show() # uses version one of the delta table (aka the second version of the table) \n",
    "\n",
    "# This should show you the original updates we did, when we overwite the whole table to change the ages of each of Jamie, Robert & Eddard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And thats the end of the example Delta Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop() # close spark session "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
